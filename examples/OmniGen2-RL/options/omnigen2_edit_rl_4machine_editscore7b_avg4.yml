name: omnigen2_edit_rl_4machine_editscore7b_avg4

seed: 2233
device_specific_seed: true
workder_specific_seed: true

reward_server_config: reward_server/server_configs/editscore_7B_avg4.yml

data:
  data_path: data_configs/train/example/train.yml
  use_chat_template: true
  maximum_text_tokens: 888
  prompt_dropout_prob: !!float 0.0
  ref_img_dropout_prob: !!float 0.0
  max_output_pixels: 262144 # 512 * 512
  max_input_pixels: [262144, 262144, 262144, 262144] # [512 * 512, 512 * 512, 512 * 512, 512 * 512]
  max_side_length: 2048
  
model:
  pretrained_vae_model_name_or_path: black-forest-labs/FLUX.1-dev
  pretrained_text_encoder_model_name_or_path: Qwen/Qwen2.5-VL-3B-Instruct
  pretrained_model_path: pretrained_models/OmniGen2/transformer/pytorch_model.bin
  
  arch_opt:
    patch_size: 2
    in_channels: 16
    hidden_size: 2520
    num_layers: 32
    num_refiner_layers: 2
    num_attention_heads: 21
    num_kv_heads: 7
    multiple_of: 256
    norm_eps: !!float 1e-05
    axes_dim_rope: [40, 40, 40]
    axes_lens: [10000, 10000, 10000]
    text_feat_dim: 2048
    timestep_scale: !!float 1000

transport:
  snr_type: lognorm
  do_shift: true
  dynamic_time_shift: true

train:
  global_batch_size: 576
  batch_size: 18
  gradient_accumulation_steps: 1

  max_train_steps: 1000
  
  dataloader_num_workers: 12

  # Optimizer
  learning_rate: !!float 4e-4
  scale_lr: false
  lr_scheduler: timm_constant_with_warmup
  warmup_t: 0
  warmup_lr_init: 1e-7
  warmup_prefix: true
  t_in_epochs: false

  # resume_from_checkpoint: 

  use_8bit_adam: false
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_weight_decay: !!float 0.01
  adam_epsilon: !!float 1e-08
  max_grad_norm: 1

  gradient_checkpointing: true
  
  set_grads_to_none: true

  # Misc
  allow_tf32: false
  mixed_precision: 'bf16'

  ema_decay: 0.0

  lora_ft: true
  lora_rank: 32
  lora_alpha: 64
  lora_dropout: 0

  rl:
    num_unique_prompts_per_sampling: 48
    num_update_steps_per_sampling: 2
    batch_size_per_forward: 9
    num_images_per_prompt: 12
    sigma_coef: 0.7
    negative_prompt: ""
    num_inference_step: 20
    max_sequence_length: 1024
    text_guidance_scale: 4
    image_guidance_scale: 2
    cfg_range_start: 0.0
    cfg_range_end: 0.6
    train_timesteps_fraction: 0.6
    reuse_samples_nums: 1
    clip_range: [!!float 1e-4, !!float 5e-4]
    adv_clip_max: !!float 5
    kl_loss_weight: !!float 0.04
    apply_cfg_in_training: true
    server_type: vlm
    use_ori_neg_prompt_template: true
    time_shift_base_res: 168
    policy_loss_reweighting: false

val:
  train_visualization_interval: 5
  num_train_visualization_samples: 3

logger:
  log_with: [wandb, tensorboard]
  # log_with: ~

  checkpointing_steps: 50
  checkpoints_total_limit: ~

cache_dir: 
resume_from_checkpoint: latest